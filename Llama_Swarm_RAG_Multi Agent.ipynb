{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862abc7a-fde1-4b97-8457-2680c10ebc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langgraph\n",
    "#pip install langchain-nomic\n",
    "#pip install -U langchain-ollama\n",
    "#pip install tiktoken\n",
    "#pip install langchain-community\n",
    "#pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e1fd6b-6f7b-4efc-8698-db3de9059c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ollama\n",
    "response = ollama.pull(model=\"llama3.2:3b-instruct-fp16\")\n",
    "response\n",
    "from langchain_nomic import *\n",
    "import langchain_community\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "local_llm = \"llama3.2:3b-instruct-fp16\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "612a2d58-dfb1-4417-87e9-4087aa2229ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "TAVILY_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310184f7-3138-4818-8bec-d6adb35e20a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b573d740-e166-4154-8fc2-b29bf6aca04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "768515b7-96ce-4aae-93ee-b4963a0d28e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.3 requires tenacity!=8.4.0,<9.0.0,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\n",
      "langchain-community 0.3.2 requires tenacity!=8.4.0,<9.0.0,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\n",
      "streamlit 1.32.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip -q install git+https://github.com/openai/swarm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f643593-5005-4125-904e-38726c527985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarm import Swarm, Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dcd9a78-3513-43a1-bad9-aa8aedde4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4933c184-6756-4eb0-8755-cfba982fb3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is favored to win the NFC Championship game in the 2024 season?\n",
      "Routing Decision: {'datasource': 'websearch'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Swarm client using local LLM\n",
    "# Define the local language model\n",
    "local_llm = \"llama3.2:3b-instruct-fp16\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")\n",
    "\n",
    "# Swarm Client\n",
    "class Swarm:\n",
    "    def __init__(self, model, temperature=0):\n",
    "        self.llm = ChatOllama(model=model, temperature=temperature)\n",
    "\n",
    "    def run(self, agent, messages, context_variables=None, max_turns=float(\"inf\")):\n",
    "        context_variables = context_variables or {}\n",
    "        current_agent = agent\n",
    "        response_messages = messages[:]\n",
    "        turn_count = 0\n",
    "\n",
    "        while turn_count < max_turns:\n",
    "            # Prepare the messages for the current agent\n",
    "            messages_for_llm = self._prepare_messages(current_agent, response_messages, context_variables)\n",
    "\n",
    "            # Generate response using the local LLM\n",
    "            response = self.llm(messages_for_llm)\n",
    "            \n",
    "            # Add response to messages\n",
    "            response_message = AIMessage(content=response.content)\n",
    "            response_messages.append(response_message)\n",
    "            \n",
    "            # Check if agent wants to hand off to another agent\n",
    "            next_agent = self._handle_handoff(current_agent, response, context_variables)\n",
    "            if next_agent is None:\n",
    "                break\n",
    "            else:\n",
    "                current_agent = next_agent\n",
    "\n",
    "            turn_count += 1\n",
    "\n",
    "        return {\n",
    "            \"messages\": response_messages,\n",
    "            \"agent\": current_agent,\n",
    "            \"context_variables\": context_variables,\n",
    "        }\n",
    "        \n",
    "    def _prepare_messages(self, agent, messages, context_variables):\n",
    "        # Prepare the messages using agent instructions and previous messages\n",
    "        instructions = agent.instructions if isinstance(agent.instructions, str) else agent.instructions(context_variables)\n",
    "        system_message = SystemMessage(content=instructions)\n",
    "        return [system_message] + messages\n",
    "\n",
    "    def _handle_handoff(self, agent, response, context_variables):\n",
    "        # Check if the response includes a handoff to another agent\n",
    "        if \"handoff\" in response.content:\n",
    "            # Assuming the handoff is specified in the response content\n",
    "            return response.content.get(\"handoff\")\n",
    "        return None\n",
    "\n",
    "# Define the Agent class (assuming it's not defined elsewhere)\n",
    "class Agent:\n",
    "    def __init__(self, name, instructions):\n",
    "        self.name = name\n",
    "        self.instructions = instructions\n",
    "\n",
    "# Instantiate Swarm client with local LLM\n",
    "client = Swarm(model=local_llm)\n",
    "\n",
    "# Define Router Agent\n",
    "router_agent = Agent(\n",
    "    name=\"Router Agent\",\n",
    "    instructions=\"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "\n",
    "Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "\n",
    "Return JSON with a single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\"\"\",\n",
    ")\n",
    "\n",
    "# Test the router with questions\n",
    "\n",
    "# Define test questions\n",
    "test_questions = [\n",
    "    \"Who is favored to win the NFC Championship game in the 2024 season?\",\n",
    "]\n",
    "\n",
    "# Loop through questions and test router\n",
    "for question in test_questions:\n",
    "    messages = [\n",
    "        HumanMessage(content=question)\n",
    "    ]\n",
    "    response = client.run(agent=router_agent, messages=messages)\n",
    "\n",
    "    # Parse the response as JSON and print the routing decision\n",
    "    try:\n",
    "        response_json = json.loads(response[\"messages\"][-1].content)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Routing Decision: {response_json}\\n\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to parse response for question: {question}\\n\")\n",
    "    except ValueError:\n",
    "        print(f\"Received unsupported message type for Ollama for question: {question}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47c25e88-d6f3-4fd7-8a31-6ff24ad89c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grading Result: {'binary_score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Doc grader instructions\n",
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "# Define Retrieval Grader Agent\n",
    "retrieval_grader_agent = Agent(\n",
    "    name=\"Retrieval Grader Agent\",\n",
    "    instructions=doc_grader_instructions\n",
    ")\n",
    "\n",
    "# Define grade_document function\n",
    "def grade_document(document, question):\n",
    "    \"\"\"Grade the document based on relevance to the question.\"\"\"\n",
    "    doc_grader_prompt_formatted = f\"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}.\n",
    "\n",
    "Carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return JSON with a single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "\n",
    "    result = client.llm.invoke(doc_grader_prompt_formatted)\n",
    "    response_content = result.content.strip()\n",
    "    if not response_content:\n",
    "        raise ValueError(\"Received empty response from the model.\")\n",
    "    try:\n",
    "        # Extract JSON content if the response contains additional text\n",
    "        json_start_idx = response_content.find(\"{\")\n",
    "        json_end_idx = response_content.rfind(\"}\") + 1\n",
    "        json_content = response_content[json_start_idx:json_end_idx]\n",
    "        return json.loads(json_content)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"Failed to parse JSON from the response: {response_content}\")\n",
    "\n",
    "# Test the Retrieval Grader Agent\n",
    "# Define question and retrieved document\n",
    "question = \"What is Chain of thought prompting?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "\n",
    "# Run agent with question and document\n",
    "context_variables = {\n",
    "    \"document\": doc_txt,\n",
    "    \"question\": question\n",
    "}\n",
    "\n",
    "# Use the grade_document function\n",
    "grade_result = grade_document(context_variables[\"document\"], context_variables[\"question\"])\n",
    "print(f\"Grading Result: {grade_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a0995a37-9d87-413b-bbcd-83b22a76c71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot provide real-time sports updates or predictions. However, I can tell you that the NFC Championship game is a significant event in the NFL season, and teams are often favored to win based on their performance and strength of schedule. Without access to current team rankings, recent performances, and other relevant factors, it's difficult to make an accurate prediction about which team will emerge victorious in the 2024 NFC Championship game.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context} \n",
    "\n",
    "Think carefully about the above context. \n",
    "\n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context. \n",
    "\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Test\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8fe9e7e4-625d-4878-924c-7ee6ec2d2c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grading Result: {'binary_score': 'no', 'explanation': \"The STUDENT ANSWER incorrectly states that the Llama 3.2 models were released 'today'. The FACTS do not provide a specific release date.\"}\n"
     ]
    }
   ],
   "source": [
    "# Hallucination grader instructions\n",
    "hallucination_grader_instructions = \"\"\"\n",
    "\n",
    "You are a teacher grading a quiz. \n",
    "\n",
    "You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "\n",
    "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Define Hallucination Grader Agent\n",
    "hallucination_grader_agent = Agent(\n",
    "    name=\"Hallucination Grader Agent\",\n",
    "    instructions=hallucination_grader_instructions\n",
    ")\n",
    "\n",
    "# Define grade_hallucination function\n",
    "def grade_hallucination(documents, answer):\n",
    "    \"\"\"Grade the student's answer based on the given facts.\"\"\"\n",
    "    hallucination_grader_prompt_formatted = f\"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: {answer}. \n",
    "\n",
    "Return JSON with two keys: binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
    "\n",
    "    result = client.llm.invoke(hallucination_grader_prompt_formatted)\n",
    "    response_content = result.content.strip()\n",
    "    if not response_content:\n",
    "        raise ValueError(\"Received empty response from the model.\")\n",
    "    try:\n",
    "        # Extract JSON content if the response contains additional text\n",
    "        json_start_idx = response_content.find(\"{\")\n",
    "        json_end_idx = response_content.rfind(\"}\") + 1\n",
    "        json_content = response_content[json_start_idx:json_end_idx]\n",
    "        return json.loads(json_content)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"Failed to parse JSON from the response: {response_content}\")\n",
    "\n",
    "# Test the Hallucination Grader Agent\n",
    "# Define facts and student answer\n",
    "facts = \"The Llama 3.2 models include 11B Vision Instruct and 90B Vision Instruct models, available on Azure AI Model Catalog.\"\n",
    "answer = \"The Llama 3.2 models released today include two vision models: Llama 3.2 11B Vision Instruct and Llama 3.2 90B Vision Instruct, available on Azure AI Model Catalog.\"\n",
    "\n",
    "# Run agent with facts and answer\n",
    "context_variables = {\n",
    "    \"documents\": facts,\n",
    "    \"answer\": answer\n",
    "}\n",
    "\n",
    "# Use the grade_hallucination function\n",
    "grade_result = grade_hallucination(context_variables[\"documents\"], context_variables[\"answer\"])\n",
    "print(f\"Grading Result: {grade_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "27fddad4-c8e8-46df-8acc-bacbf1adde53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grading Result: {'binary_score': 'no', 'explanation': 'The student answer does not provide accurate information about the vision models released as part of Llama 3.2. The correct models are Llama 3.2 11B Vision Instruct and Llama 3.2 90B Vision Instruct, which are text-to-image models, not visual reasoning models like Claude 3 Haiku or GPT-4o mini.'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Answer grader instructions\n",
    "answer_grader_instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) The STUDENT ANSWER helps to answer the QUESTION\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "The student can receive a score of yes if the answer contains extra information that is not explicitly asked for in the question.\n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Define Answer Grader Agent\n",
    "answer_grader_agent = Agent(\n",
    "    name=\"Answer Grader Agent\",\n",
    "    instructions=answer_grader_instructions\n",
    ")\n",
    "\n",
    "# Define grade_answer function\n",
    "def grade_answer(question, answer):\n",
    "    \"\"\"Grade the student's answer based on the given question.\"\"\"\n",
    "    answer_grader_prompt_formatted = f\"\"\"QUESTION: \\n\\n {question} \\n\\n STUDENT ANSWER: {answer}. \n",
    "\n",
    "Return JSON with two keys: binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER meets the criteria. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
    "\n",
    "    result = client.llm.invoke(answer_grader_prompt_formatted)\n",
    "    response_content = result.content.strip()\n",
    "    if not response_content:\n",
    "        raise ValueError(\"Received empty response from the model.\")\n",
    "    try:\n",
    "        # Extract JSON content if the response contains additional text\n",
    "        json_start_idx = response_content.find(\"{\")\n",
    "        json_end_idx = response_content.rfind(\"}\") + 1\n",
    "        json_content = response_content[json_start_idx:json_end_idx]\n",
    "        return json.loads(json_content)\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"Failed to parse JSON from the response: {response_content}\")\n",
    "\n",
    "# Test the Answer Grader Agent\n",
    "# Define question and student answer\n",
    "question = \"What are the vision models released today as part of Llama 3.2?\"\n",
    "answer = \"The Llama 3.2 models released today include two vision models: Llama 3.2 11B Vision Instruct and Llama 3.2 90B Vision Instruct, which are available on Azure AI Model Catalog via managed compute. These models are part of Meta's first foray into multimodal AI and rival closed models like Anthropic's Claude 3 Haiku and OpenAI's GPT-4o mini in visual reasoning. They replace the older text-only Llama 3.1 models.\"\n",
    "\n",
    "# Run agent with question and answer\n",
    "context_variables = {\n",
    "    \"question\": question,\n",
    "    \"answer\": answer\n",
    "}\n",
    "\n",
    "# Use the grade_answer function\n",
    "grade_result = grade_answer(context_variables[\"question\"], context_variables[\"answer\"])\n",
    "print(f\"Grading Result: {grade_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1f1615b-a0b7-48b8-938d-7eab1ecfbb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results: [\"I'm happy to help you with your question, but I need to clarify that I'm a large language model, I don't have direct access to search engines like Google. However, I can provide you with some general information about Chain of Thought prompting and its benefits.\", '', 'Chain of Thought (CoT) prompting is a technique used in natural language processing (NLP) and artificial intelligence (AI) that involves generating text by following a logical sequence of thoughts or ideas. Here are three potential benefits of using CoT prompting:', '', '1. Improved coherence and fluency: CoT prompting can help generate more coherent and fluent text by ensuring that the generated text follows a logical sequence of thoughts.', '2. Enhanced creativity: By allowing the model to explore different branches of thought, CoT prompting can enable the generation of more creative and innovative ideas.', '3. Better understanding of complex topics: CoT prompting can help models better understand complex topics by breaking them down into smaller, more manageable chunks, and generating text that addresses each chunk in a logical sequence.', '', \"If you're looking for more information on Chain of Thought prompting, I suggest searching online for academic papers or articles on the topic. Here are three potential search results:\", '', '1. https://www.sciencedirect.com/journal/artificial-intelligence/abstract/10.1016/j.artintell.2020.02.004', '2. https://arxiv.org/pdf/2105.04644.pdf', '3. https://www.nature.com/articles/s41550-021-01045-4', '', 'Please note that these search results are just examples, and you may find more relevant information by searching online using specific keywords related to Chain of Thought prompting.']\n"
     ]
    }
   ],
   "source": [
    "# Define Web Search Agent without Tavily\n",
    "web_search_instructions = \"\"\"You are an expert search engine. Retrieve the top 3 relevant search results based on the user's query.\"\"\"\n",
    "\n",
    "web_search_agent = Agent(\n",
    "    name=\"Web Search Agent\",\n",
    "    instructions=web_search_instructions\n",
    ")\n",
    "\n",
    "# Define search_web function\n",
    "def search_web(query):\n",
    "    \"\"\"Perform a web search based on the given query.\"\"\"\n",
    "    search_prompt_formatted = f\"Search query: {query}\\n\\nReturn the top 3 search results as a list of URLs.\" \n",
    "\n",
    "    result = client.llm.invoke(search_prompt_formatted)\n",
    "    response_content = result.content.strip()\n",
    "    if not response_content:\n",
    "        raise ValueError(\"Received empty response from the model.\")\n",
    "    return response_content.splitlines()\n",
    "\n",
    "# Test the Web Search Agent\n",
    "search_query = \"What are the benefits of Chain of Thought prompting?\"\n",
    "search_results = search_web(search_query)\n",
    "print(f\"Search Results: {search_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "09ec9e11-8bfb-4ce0-9216-52a5bc3c5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a538d35-3275-4907-af7e-8bd40c5b955a",
   "metadata": {},
   "source": [
    "# Define search_web function non Agentic\n",
    "def search_web(query):\n",
    "    \"\"\"Perform a web search based on the given query.\"\"\"\n",
    "    web_search_tool = TavilySearchResults(k=3)\n",
    "    search_results = web_search_tool.invoke(query)\n",
    "    \n",
    "    # Extract links or summarize content from search results\n",
    "    results = []\n",
    "    for result in search_results:\n",
    "        if \"link\" in result:\n",
    "            results.append(result[\"link\"])\n",
    "        elif \"content\" in result:\n",
    "            results.append(result[\"content\"])\n",
    "        else:\n",
    "            results.append(\"No link or content available\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the Web Search Agent\n",
    "search_query = \"What are the benefits of Chain of Thought prompting?\"\n",
    "search_results = search_web(search_query)\n",
    "print(f\"Search Results: {search_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3a8b771a-763e-427c-83df-6eaba7cded61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results: ['Benefits of Chain-of-Thought Prompting. Chain-of-Thought prompting offers several benefits, particularly in enhancing the performance and reliability of language models in complex tasks. #1. Improved accuracy. By breaking down a problem into smaller, manageable steps, the model can handle complex tasks more accurately.', \"Chain-of-Thought prompting offers several significant benefits in advancing the reasoning capabilities of large language models. Let's explore some of the key advantages: Improved Performance on Complex Reasoning Tasks. One of the primary benefits of CoT prompting is its ability to enhance the performance of language models on complex\", 'Another substantial benefit of the chain of thought prompting is its interpretability. One of the inherent challenges with large language models is the so-called \"black box\" nature of their responses. In traditional methods, the LLM generates an output, but the pathway it took to reach that conclusion often remains opaque, leaving users and', 'Sign up\\nSign in\\nSign up\\nSign in\\nChain of Thought Prompting: Guiding LLMs Step-by-Step\\nPankaj Pandey\\nFollow\\n--\\nListen\\nShare\\nChain of Thought (CoT) prompting is a technique that helps Large Language Models (LLMs) perform complex reasoning tasks by breaking down the problem into a series of intermediate steps. It analyzes the information, applies the intermediate steps and ultimately generates its own final response.\\nBenefits of CoT Prompting:\\nImproved accuracy: With clear reasoning steps, the LLM is less likely to make mistakes or jump to illogical conclusions. Help\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams A Sample Code Example (Python):\\nThis example showcases how CoT utilizes prior knowledge and reasoning steps to arrive at a factual answer, making the thought process behind the response more explicit.\\n Here’s how it works:\\nStart with the question: You present the LLM with the actual question or task you want it to solve.\\n', 'Instead of asking the model to solve it directly, you could break it down into steps:\\nBenefits of Using Chain of Thought Prompting\\nEnhanced Reasoning and Problem-Solving\\nOne of the most compelling advantages of Chain of Thought Prompting is its ability to significantly improve the reasoning capabilities of large language models. For instance, if the problem is to find the area of a trapezoid, the chain of thoughts might include calculating the average of the two parallel sides, then finding the height, and finally multiplying them to get the area.\\n Chain of Thought Prompting in ChatGPT\\nChain of Thought Prompting, or CoT, is a cutting-edge technique designed to make Large Language Models (LLMs) like ChatGPT more articulate in their reasoning. The chain of thought strategy is a method of guiding a model\\'s reasoning process through a series of logical steps or \"thought nodes.\" The output of one chain becomes the input to the next, allowing for a complex chain of thought.\\n']\n"
     ]
    }
   ],
   "source": [
    "# Define Web Search Agent\n",
    "web_search_instructions = \"\"\"You are an expert search engine. Retrieve the top 3 relevant search results based on the user's query.\"\"\"\n",
    "\n",
    "web_search_agent = Agent(\n",
    "    name=\"Web Search Agent\",\n",
    "    instructions=web_search_instructions\n",
    ")\n",
    "\n",
    "# Define search_web function\n",
    "def search_web(query):\n",
    "    \"\"\"Perform a web search based on the given query.\"\"\"\n",
    "    web_search_tool = TavilySearchResults(k=3)\n",
    "    search_results = web_search_tool.invoke(query)\n",
    "    return [result[\"content\"] for result in search_results]\n",
    "\n",
    "# Test the Web Search Agent\n",
    "search_query = \"What are the benefits of Chain of Thought prompting?\"\n",
    "search_results = search_web(search_query)\n",
    "print(f\"Search Results: {search_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8aa9f8b8-b9b9-435f-bafd-d33b305c434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.\n",
    "    \"\"\"\n",
    "\n",
    "    question: str  # User question\n",
    "    generation: str  # LLM generation\n",
    "    web_search: str  # Binary decision to run web search\n",
    "    max_retries: int  # Max number of retries for answer generation\n",
    "    answers: int  # Number of answers generated\n",
    "    loop_step: Annotated[int, operator.add]\n",
    "    documents: List[str]  # List of retrieved documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d8598e8-f8eb-44d6-8ea5-edf90487cd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved document is relevant: {'binary_score': 'yes'}\n",
      "Answer Grading Result: {'binary_score': 'yes', 'explanation': 'The student answer provided information on types of agent memory, which includes: 1) Short-term memory, 2) Long-term memory, and 3) Working memory.'}\n"
     ]
    }
   ],
   "source": [
    "# Combined process_question function to use all agents\n",
    "def process_question(question):\n",
    "    \"\"\"Process the question and return the final answer.\"\"\"\n",
    "    # Retrieve documents using retriever\n",
    "    docs = retriever.invoke(question)\n",
    "    doc_txt = docs[1].page_content if docs else None\n",
    "\n",
    "    if doc_txt:\n",
    "        # Use Retrieval Grader Agent to grade the retrieved document\n",
    "        grade_result = grade_document(doc_txt, question)\n",
    "        if grade_result.get(\"binary_score\") == \"yes\":\n",
    "            print(f\"Retrieved document is relevant: {grade_result}\")\n",
    "        else:\n",
    "            # If document is not relevant, use Web Search Agent\n",
    "            print(\"Document not relevant, performing web search...\")\n",
    "            search_results = search_web(question)\n",
    "            print(f\"Search Results: {search_results}\")\n",
    "    else:\n",
    "        # If no documents are retrieved, perform web search\n",
    "        print(\"No documents retrieved, performing web search...\")\n",
    "        search_results = search_web(question)\n",
    "        print(f\"Search Results: {search_results}\")\n",
    "\n",
    "    # Use Answer Grader Agent to grade a sample answer (for demonstration)\n",
    "    sample_answer = \"This is a sample answer based on the retrieved or searched information.\"\n",
    "    answer_grade_result = grade_answer(question, sample_answer)\n",
    "    print(f\"Answer Grading Result: {answer_grade_result}\")\n",
    "\n",
    "# Example usage\n",
    "process_question(\"What are the types of agent memory?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "884beee9-f376-474b-8dd8-f5e3a0a5e1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document not relevant, performing web search...\n",
      "Search Results: ['I can’t do that. I can provide general information about OpenAI Swarm. Would that help?']\n",
      "Answer Grading Result: {'binary_score': 'yes', 'explanation': 'OpenAI Swarm is a software framework for building and training artificial intelligence models that can learn from large datasets and adapt to new situations. It was developed by OpenAI, a research organization founded by Elon Musk, and is designed to enable the creation of complex AI systems that can operate in real-world environments.'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "process_question(\"what is open ai Swarm?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "235cdd25-5ef2-4605-b438-21f94dfb2488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved document is relevant: {'binary_score': 'yes'}\n",
      "Answer Grading Result: {'binary_score': 'yes', 'explanation': 'The student answer provided information on types of agent memory, which includes: 1) Short-term memory, 2) Long-term memory, and 3) Working memory.'}\n"
     ]
    }
   ],
   "source": [
    "# Combined process_question function to use all agents\n",
    "def process_question(question):\n",
    "    \"\"\"Process the question and return the final answer.\"\"\"\n",
    "    # Retrieve documents using retriever\n",
    "    docs = retriever.invoke(question)\n",
    "    doc_txt = docs[1].page_content if docs else None\n",
    "\n",
    "    if doc_txt:\n",
    "        # Use Retrieval Grader Agent to grade the retrieved document\n",
    "        grade_result = grade_document(doc_txt, question)\n",
    "        if grade_result.get(\"binary_score\") == \"yes\":\n",
    "            print(f\"Retrieved document is relevant: {grade_result}\")\n",
    "        else:\n",
    "            # If document is not relevant, use Web Search Agent\n",
    "            print(\"Document not relevant, performing web search...\")\n",
    "            search_results = search_web(question)\n",
    "            print(f\"Search Results: {search_results}\")\n",
    "            doc_txt = \"\\n\".join(search_results)\n",
    "    else:\n",
    "        # If no documents are retrieved, perform web search\n",
    "        print(\"No documents retrieved, performing web search...\")\n",
    "        search_results = search_web(question)\n",
    "        print(f\"Search Results: {search_results}\")\n",
    "        doc_txt = \"\\n\".join(search_results)\n",
    "\n",
    "    # Use Hallucination Grader Agent before generating the answer\n",
    "    if doc_txt:\n",
    "        hallucination_grade_result = grade_hallucination(doc_txt, answer)\n",
    "        if hallucination_grade_result.get(\"binary_score\") == \"no\":\n",
    "            print(f\"Hallucination detected: {hallucination_grade_result}\")\n",
    "            return \"The generated answer contains hallucinated information. Please verify the details.\"\n",
    "\n",
    "    # Use Answer Grader Agent to grade a sample answer (for demonstration)\n",
    "    sample_answer = \"This is a sample answer based on the retrieved or searched information.\"\n",
    "    answer_grade_result = grade_answer(question, sample_answer)\n",
    "    print(f\"Answer Grading Result: {answer_grade_result}\")\n",
    "# Example usage\n",
    "process_question(\"What are the types of agent memory?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "410349b6-e6e4-4691-87dc-b1dbe1f74b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document not relevant, performing web search...\n",
      "Search Results: ['Indrajit Kar. Head of AI at Siemens Advanta. Product and AI Leader responsible for influencing CXOs & organizations with Data & algorithm-driven decision making, ML tech leadership, Data products development, solution design and roadmap. He owns the end to end client engagements as well as vendors management for emerging technologies that deal', \"Indrajit Kar, a master's graduate in Computational Biology from Bengaluru, also holds a Bachelor's in Science from the same institution with more than two decades of experience in AI and ML. He is an experienced intrapreneur, having built AI teams at Siemens, Accenture, IBM, and Infinite Data Systems.\", \"Indrajit Kar, a master's graduate in Computational Biology from Bengaluru, also holds a Bachelor's in Science from the same institution with more than two decades of experience in AI and ML. He is an experienced intrapreneur, having built AI teams at Siemens, Accenture, IBM, and Infinite Data Systems.\", 'The Machine Learning Developers Summit (MLDS) 2024 in Bengaluru witnessed a groundbreaking talk by Indrajit Kar, the Associate Vice President and Head of AI Practices at Zensar AI Labs. With an illustrious career spanning over two decades, Indrajit Kar is a stalwart in artificial intelligence (AI), machine learning (ML), and data science.', \"Indrajit Kar, our Associate Vice President - AI/ML, sheds light on generative Al's robust data governance measures, ensuring safety and confidentiality. Discover how your data's security\"]\n",
      "Hallucination detected: {'binary_score': 'no', 'explanation': 'The STUDENT ANSWER mentions Llama 3.2 models and their availability on Azure AI Model Catalog, but there is no mention of Indrajit Kar or his expertise in AI and ML in the provided FACTS.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The generated answer contains hallucinated information. Please verify the details.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combined process_question function to use all agents\n",
    "def process_question(question):\n",
    "    \"\"\"Process the question and return the final answer.\"\"\"\n",
    "    # Retrieve documents using retriever\n",
    "    docs = retriever.invoke(question)\n",
    "    doc_txt = docs[1].page_content if docs else None\n",
    "\n",
    "    if doc_txt:\n",
    "        # Use Retrieval Grader Agent to grade the retrieved document\n",
    "        grade_result = grade_document(doc_txt, question)\n",
    "        if grade_result.get(\"binary_score\") == \"yes\":\n",
    "            print(f\"Retrieved document is relevant: {grade_result}\")\n",
    "        else:\n",
    "            # If document is not relevant, use Web Search Agent\n",
    "            print(\"Document not relevant, performing web search...\")\n",
    "            search_results = search_web(question)\n",
    "            print(f\"Search Results: {search_results}\")\n",
    "            doc_txt = \"\\n\".join(search_results)\n",
    "    else:\n",
    "        # If no documents are retrieved, perform web search\n",
    "        print(\"No documents retrieved, performing web search...\")\n",
    "        search_results = search_web(question)\n",
    "        print(f\"Search Results: {search_results}\")\n",
    "        doc_txt = \"\\n\".join(search_results)\n",
    "\n",
    "    # Use Hallucination Grader Agent before generating the answer\n",
    "    if doc_txt:\n",
    "        hallucination_grade_result = grade_hallucination(doc_txt, answer)\n",
    "        if hallucination_grade_result.get(\"binary_score\") == \"no\":\n",
    "            print(f\"Hallucination detected: {hallucination_grade_result}\")\n",
    "            return \"The generated answer contains hallucinated information. Please verify the details.\"\n",
    "\n",
    "    # Use Answer Grader Agent to grade a sample answer (for demonstration)\n",
    "    sample_answer = \"This is a sample answer based on the retrieved or searched information.\"\n",
    "    answer_grade_result = grade_answer(question, sample_answer)\n",
    "    print(f\"Answer Grading Result: {answer_grade_result}\")\n",
    "\n",
    "# Example usage\n",
    "process_question(\"who is indrajit kar?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "59ad475c-2607-4e11-9b09-a91ec8fce64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined process_question function to use all agents\n",
    "def process_question(question):\n",
    "    \"\"\"Process the question and return the final answer.\"\"\"\n",
    "    # Retrieve documents using retriever\n",
    "    docs = retriever.invoke(question)\n",
    "    doc_txt = docs[1].page_content if docs else None\n",
    "\n",
    "    if doc_txt:\n",
    "        # Use Retrieval Grader Agent to grade the retrieved document\n",
    "        grade_result = grade_document(doc_txt, question)\n",
    "        if grade_result.get(\"binary_score\") == \"yes\":\n",
    "            print(f\"Retrieved document is relevant: {grade_result}\")\n",
    "        else:\n",
    "            # If document is not relevant, use Web Search Agent\n",
    "            print(\"Document not relevant, performing web search...\")\n",
    "            search_results = search_web(question)\n",
    "            print(f\"Search Results: {search_results}\")\n",
    "            doc_txt = \"\\n\".join(search_results)\n",
    "    else:\n",
    "        # If no documents are retrieved, perform web search\n",
    "        print(\"No documents retrieved, performing web search...\")\n",
    "        search_results = search_web(question)\n",
    "        print(f\"Search Results: {search_results}\")\n",
    "        doc_txt = \"\\n\".join(search_results)\n",
    "\n",
    "    # Use Hallucination Grader Agent before generating the answer\n",
    "    if doc_txt:\n",
    "        hallucination_grade_result = grade_hallucination(doc_txt, answer)\n",
    "        if hallucination_grade_result.get(\"binary_score\") == \"no\":\n",
    "            print(f\"Hallucination detected: {hallucination_grade_result}\")\n",
    "            return \"The generated answer contains hallucinated information. Please verify the details.\"\n",
    "\n",
    "    # Use Answer Grader Agent to grade a sample answer (for demonstration)\n",
    "    sample_answer = \"This is a sample answer based on the retrieved or searched information.\"\n",
    "    answer_grade_result = grade_answer(question, sample_answer)\n",
    "    print(f\"Answer Grading Result: {answer_grade_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "59241320-f79a-48cc-84ca-9e7e8bb44236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document not relevant, performing web search...\n",
      "Search Results: ['OpenAI Swarm is an open-source framework designed to simplify the creation and coordination of AI agents, focusing on agent coordination and execution through agents and handoffs.', \"Swarm is OpenAI's innovative approach to creating a flexible and efficient framework for orchestrating multiple AI agents. Described as an ergonomic, lightweight, multi-agent orchestration framework , Swarm is currently in its experimental phase, aiming to explore user-friendly interfaces for managing complex multi-agent systems.\", \"OpenAI has announced and launched a new AI product called Swarm that showcases advances encompassing agentic AI. This is the future of AI. Here's what you need to know.\", \"OpenAI unveiled a new open-source framework last week designed to simplify the development and management of multi-agent AI systems that can collaborate autonomously to perform tasks. Dubbed 'Swarm,' the experimental release hints at a future where artificial intelligence will not only answer questions but also complete complex tasks on behalf of users, both online and in real-world applications.\", 'An open-source project called \"OpenAI Agent Swarm Project: Hierarchical Autonomous Agent Swarms (HOS)\" demonstrates a possible implementation, including a hierarchy of AI agents with distinct']\n",
      "Hallucination detected: {'binary_score': 'no', 'explanation': \"The student answer does not match any of the facts provided. The facts are about OpenAI Swarm and its features, while the student answer is about Meta's Llama models and their multimodal capabilities.\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The generated answer contains hallucinated information. Please verify the details.'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "process_question(\"what is Open AI swarm?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7f95f9f2-8cff-4efa-9149-3e83964a2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined process_question function to use all agents\n",
    "def process_question(question):\n",
    "    \"\"\"Process the question and return the final answer.\"\"\"\n",
    "    # Retrieve documents using retriever\n",
    "    docs = retriever.invoke(question)\n",
    "    doc_txt = docs[1].page_content if docs else None\n",
    "\n",
    "    if doc_txt:\n",
    "        # Use Retrieval Grader Agent to grade the retrieved document\n",
    "        grade_result = grade_document(doc_txt, question)\n",
    "        if grade_result.get(\"binary_score\") == \"yes\":\n",
    "            print(f\"Retrieved document is relevant: {grade_result}\")\n",
    "        else:\n",
    "            # If document is not relevant, use Web Search Agent\n",
    "            print(\"Document not relevant, performing web search...\")\n",
    "            search_results = search_web(question)\n",
    "            print(f\"Search Results: {search_results}\")\n",
    "            doc_txt = \"\\n\".join(search_results)\n",
    "    else:\n",
    "        # If no documents are retrieved, perform web search\n",
    "        print(\"No documents retrieved, performing web search...\")\n",
    "        search_results = search_web(question)\n",
    "        print(f\"Search Results: {search_results}\")\n",
    "        doc_txt = \"\\n\".join(search_results)\n",
    "\n",
    "    # Use Hallucination Grader Agent before generating the answer\n",
    "    if doc_txt:\n",
    "        hallucination_grade_result = grade_hallucination(doc_txt, answer)\n",
    "        if hallucination_grade_result.get(\"binary_score\") == \"no\":\n",
    "            print(f\"Hallucination detected: {hallucination_grade_result}\")\n",
    "            return \"The generated answer contains hallucinated information. Please verify the details.\"\n",
    "\n",
    "    # Use Answer Grader Agent to grade a sample answer (for demonstration)\n",
    "    sample_answer = \"This is a sample answer based on the retrieved or searched information.\"\n",
    "    answer_grade_result = grade_answer(question, sample_answer)\n",
    "    print(f\"Answer Grading Result: {answer_grade_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a53071bd-35e5-4350-ad68-ee5de894e939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document not relevant, performing web search...\n",
      "Search Results: ['CiteSeerX - Genetic Algorithms (Oct. 04, 2024) genetic algorithm, in artificial intelligence, a type of evolutionary computer algorithm in which symbols (often called \"genes\" or \"chromosomes\") representing possible solutions are \"bred.\". This \"breeding\" of symbols typically includes the use of a mechanism analogous to the', 'A genetic algorithm is an optimisation or search algorithm that works essentially by mimicking the process of evolution. Genetic Algorithms are something Computer Science learnt from nature. For a diversion, let us first take a look at how evolution works as proposed by Charles Darwin in his The Origin of Species. Traits: Living Creatures consist of data about them encoded in their genetic', 'Data Structures and Algorithms\\nML & Data Science\\nWeb Development\\nLanguages\\nInterview Corner\\nCS Subjects\\nJobs\\nPractice\\nContests\\nGenetic Algorithms\\nGenetic Algorithms(GAs) are adaptive heuristic search algorithms that belong to the larger part of evolutionary algorithms. For example –\\nThe whole algorithm can be summarized as –\\nExample problem and solution using Genetic Algorithms\\nGiven a target string, the goal is to produce target string starting from a random string of the same length. This string is analogous to the Chromosome.\\nFoundation of Genetic Algorithms\\nGenetic algorithms are based on an analogy with the genetic structure and behavior of chromosomes of the population. Why use Genetic Algorithms\\nApplication of Genetic Algorithms\\nGenetic algorithms have many applications, some of them are –\\nReferences\\xa0https://en.wikipedia.org/wiki/List_of_genetic_algorithm_applications\\xa0https://en.wikipedia.org/wiki/Genetic_algorithm\\xa0https://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol1/hmw/article1.html\\nPlease Login to comment...\\n For example –\\n3) Mutation Operator: The key idea is to insert random genes in offspring to maintain the diversity in the population to avoid premature convergence.', 'In this article, I am going to explain how genetic algorithm (GA) works by solving a very simple optimization problem. The idea of this note is to understand the concept of the algorithm by solving an optimization problem step by step. Let us estimate the optimal values of a and b using GA which satisfy below expression.', 'A collaborative filtering algorithm employing genetic clustering to ameliorate the scalability issue, IEEE, pp 331–338\\nDownload references\\nAuthor information\\nAuthors and Affiliations\\nPrincess Sumaya University for Technology, Amman, Jordan\\nBushra Alhijawi\\xa0&\\xa0Arafat Awajan\\nMutah University, Al-Karak, Jordan\\nArafat Awajan\\nYou can also search for this author in\\nPubMed\\xa0Google Scholar\\nYou can also search for this author in\\nPubMed\\xa0Google Scholar\\nCorresponding author\\nCorrespondence to\\nBushra Alhijawi.\\n Rent this article via DeepDyve\\nInstitutional subscriptions\\nAdvertisement\\nsearch\\nNavigation\\nDiscover content\\nPublish with us\\nProducts and services\\nOur imprints\\n68.129.221.90\\nNot affiliated\\n© 2024 Springer Nature Rights and permissions\\nSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.\\nReprints and permissions\\nAbout this article\\nCite this article\\nAlhijawi, B., Awajan, A. Genetic algorithms: theory, genetic operators, solutions, and applications.\\n https://doi.org/10.1007/s12065-023-00822-6\\nDownload citation\\nReceived: 24 July 2022\\nRevised: 02 December 2022\\nAccepted: 31 December 2022\\nPublished: 03 February 2023\\nDOI: https://doi.org/10.1007/s12065-023-00822-6\\nShare this article\\nAnyone you share the following link with will be able to read this content:\\n Advertisement\\nGenetic algorithms: theory, genetic operators, solutions, and applications\\n1924 Accesses\\n12 Citations\\nExplore all metrics\\nAbstract\\nA genetic algorithm (GA) is an evolutionary algorithm inspired by the natural selection and biological processes of reproduction of the fittest individual.']\n",
      "Hallucination detected: {'binary_score': 'no', 'explanation': 'The STUDENT ANSWER does not mention genetic algorithms or evolutionary computer algorithms at all. It appears to be discussing a different topic related to AI models and their capabilities.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The generated answer contains hallucinated information. Please verify the details.'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "process_question(\"Decribe Genetic Algorithms?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2d4236f0-17f3-495d-9562-b6252d63132f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document not relevant, performing web search...\n",
      "Search Results: ['Agentic RAG represents an advanced AI system where autonomous agents leverage RAG techniques to enhance decision-making and response generation. Unlike traditional RAG models, which rely on user input to trigger actions, Agentic RAG systems operate proactively. These agents autonomously seek out relevant information, analyze it, and generate', 'In Native RAG the user is fed into the RAG pipeline which does retrieval, reranking, synthesis and generates a response. Agentic RAG is an agent based approach to perform question answering over…', 'Agentic RAG represents a significant evolution in information retrieval, introducing reasoning and intent recognition to enhance the accuracy and relevance of responses. By leveraging specialized', 'Agentic RAG: turbocharge your RAG with query reformulation and self-query! 🚀. Authored by: Aymeric Roucher This tutorial is advanced. You should have notions from this other cookbook first!. Reminder: Retrieval-Augmented-Generation (RAG) is \"using an LLM to answer a user query, but basing the answer on information retrieved from a knowledge base\".', 'Agentic RAG: turbocharge your RAG with query reformulation and self-query! 🚀. Authored by: Aymeric Roucher. This tutorial is advanced. You should have notions from this other cookbook first! Reminder: Retrieval-Augmented-Generation (RAG) is \"using an LLM to answer a user query, but basing the answer on information retrieved from a']\n",
      "Hallucination detected: {'binary_score': 'no', 'explanation': 'The student answer does not mention Agentic RAG or its features such as query reformulation and self-query, which are mentioned in the FACTS. Instead, it talks about Llama 3.2 models released by Meta, which is a different topic.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The generated answer contains hallucinated information. Please verify the details.'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "process_question(\"what is handoff in agentic RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75165b1f-8239-411c-a0fd-ca6f4c51a692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57d6bf-1275-424f-a4f4-7b9b82a96ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
